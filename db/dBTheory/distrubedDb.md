# Распределенные БД

## Репликация и шардинг и партиционирование

### Пар­ти­ци­о­ни­ро­ва­ние (partitioning)

Пар­ти­ци­о­ни­ро­ва­ние(также секционирование) — это раз­би­е­ние таб­лиц, содер­жа­щих боль­шое коли­че­ство запи­сей, на логи­че­ские части по неким выбран­ным адми­ни­стра­то­ром кри­те­ри­ям. Пар­ти­ци­о­ни­ро­ва­ние таб­лиц делит весь объем опе­ра­ций по обра­ботке дан­ных на несколько неза­ви­си­мых и парал­лельно выпол­ня­ю­щихся пото­ков, что суще­ственно уско­ряет работу СУБД. Для пра­виль­ного кон­фи­гу­ри­ро­ва­ния пара­мет­ров пар­ти­ци­о­ни­ро­ва­ния необ­хо­ди­мо, чтобы в каж­дом потоке было при­мерно оди­на­ко­вое коли­че­ство запи­сей.

*Напри­мер, на новост­ных сай­тах имеет смысл пар­ти­ци­о­ни­ро­вать записи по дате пуб­ли­ка­ции, так как све­жие ново­сти на несколько поряд­ков более вос­тре­бо­ваны и чаще тре­бу­ется работа именно с ними, а не со всех архи­вом за годы суще­ство­ва­ния новост­ного ресурса.*

### Репли­ка­ция (replication)

Репли­ка­ция — это син­хрон­ное или асин­хрон­ное копи­ро­ва­ние дан­ных между несколь­кими сер­ве­ра­ми. Веду­щие сер­вера назы­вают масте­рами (master), а ведо­мые сер­вера — слэй­вами (slave). Мастера исполь­зу­ются для изме­не­ния дан­ных, а слэйвы — для счи­ты­ва­ния. В клас­си­че­ской схеме репли­ка­ции обычно один мастер и несколько слэй­вов, так как в боль­шей части веб-про­ек­тов опе­ра­ций чте­ния на несколько поряд­ков боль­ше, чем опе­ра­ций запи­си. Однако в более слож­ной схеме репли­ка­ции может быть и несколько масте­ров.

Напри­мер, созда­ние несколь­ких допол­ни­тель­ных slave-сер­ве­ров поз­во­ляет снять с основ­ного сер­вера нагрузку и повы­сить общую про­из­во­ди­тель­ность систе­мы, а также можно орга­ни­зо­вать слэйвы под кон­крет­ные ресур­соём­кие задачи и таким обра­зом, напри­мер, упро­стить состав­ле­ние серь­ёз­ных ана­ли­ти­че­ских отчётов — исполь­зу­е­мый для этих целей slave может быть нагру­жен на 100%, но на работу дру­гих поль­зо­ва­те­лей при­ло­же­ния это не повли­я­ет.

Существует также схема Master-Master в ней любой из серверов может использоваться как для чтения так и для записи. Но вероятные поломки делают Master-Master репликацию непривлекательной. Выход из строя одного из серверов практически всегда приводит к потере каких-то данных. Последующее восстановление также сильно затрудняется необходимостью ручного анализа данных, которые успели либо не успели скопироваться.

Следует отметить, что репликация сама по себе не очень удобный механизм масштабирования. Причиной тому — рассинхронизация данных и задержки в копировании с мастера на слейв. Зато это отличное средство для обеспечения отказоустойчивости. Вы всегда можете переключиться на слейв, если мастер ломается и наоборот. Чаще всего репликация используется совместно с шардингом именно из соображений надежности.

#### Проблемы, решаемые репликацией

- **Распространение данных**. Обычно репликация в MySQL потребляет не очень большую часть пропускной способности сети 1 , к тому же ее можно в любой момент остановить и затем возобновить. Это полезно, если хранение копии данных происходит в географически удаленном пункте, например в другом центре обработки данных. Удаленный подчиненный сервер может работать даже с непостоянным (намеренно или по другим причинам) соединением. Однако если вы хотите обеспечить минимальное отставание реплики, то следует использовать надежный канал с малым временем задержки.
- **Балансировка нагрузки**. С помощью репликации можно распределить запросы на чтение между несколькими серверами MySQL; в приложениях с интенсивным чтением эта тактика работает очень хорошо. Реализовать несложное балансирование нагрузки можно, внеся совсем немного изменений в код. Для небольших приложений достаточно просто «зашить» в программу несколько доменных имен или воспользоваться циклическим (round-robin) разрешением DNS-имен (когда с однимдоменным именем связано несколько IP-адресов). Возможны и более изощренные решения. Стандартные технологии балансирования нагрузки, в частности сетевые балансировщики, прекрасно послужат для распределения нагрузки между несколькими серверами MySQL.
- **Резервное копирование**.Репликация – это ценное подспорье для резервного копирования.Однако подчиненный сервер все же не может использоваться в качестве резервной копии и не является заменой настоящему резервному копированию.
- **Высокая доступность и аварийное переключение на резервный сервер(failover)**. Репликация позволяет исправить ситуацию, при которой сервер MySQL является единственной точкой отказа приложения. Хорошая сис тема аварийного переключения при отказе, имеющая в составе реплицированные подчиненные серверы, способна существенно сократить время простоя. 
- **Тестирование новых версий MySQL**. Очень часто на подчиненный сервер устанавливают новую версию MySQL и перед тем как ставить ее на промышленные серверы, проверяют, что все запросы работают нормально.

####  Как работает репликация

- Главный сервер записывает изменения данных в двоичный журнал.Эти записи называются событиями двоичного журнала.  Бывает покомандная и построчная, Mysql может сама выбирать какую заюзать в конкретном случае.
  - **Покомандная** -  протоколируются все выполненные главным сервером команды изменения данных. Когда подчиненный сервер читает из своего журнала ретрансляции событие и воспроизводит его, на самом деле он отрабатывает в точности ту же команду, которая была ранее выполнена на главном сервере. В этом есть как плюсы так и минусы. Некоторые изменения, хранящиеся в формате построчной репликации, MySQL воспроизводит более эффективно, так как ему не приходится повторять запросы, выполненные на главном сервере. А ведь воспроизведение определенных запросов обходится весьма дорого.
  - **Построчная** - в двоичный журнал записывает фактические изменения данных Самое существенное достоинство заключается в том, что теперь MySQL может корректно реплицировать любую команду, причем в некоторых случаях это происходит гораздо более эффективно. Основной недостаток – это то, что двоичный журнал стал намного больше и из него непонятно, какие команды привели к обновлению данных, так что использовать его для аудита с помощью программы mysqlbinlog уже невозможно. Команды не включаются в журнал событий, поэтому будет сложно определить, какая команда выполнялась. Во многих случаях знать это так же важно, как и знать об изменении строк. актически процесс применения изменений при построчной репликации в значительной степени является черным ящиком — не видно, что делает сервер. Кроме того, он плохо документирован и объяснен, поэтому, когда что-то работает неправильно, устранить неполадки довольно сложно. Например, если подчиненный сервер выберет неэффективный способ поиска строк для изменения, вы этого не заметите.
- Подчиненный сервер копирует события двоичного журнала в свой журнал ретрансляции (relay log).
- Подчиненный сервер воспроизводит события из журнала ретрансляции, применяя изменения к собственным данным.

  ![](../../media/masterSlaveReplication.png)

### Шардинг (sharding)

Шардинг — это прием, который позволяет рас­пре­де­лять дан­ные между раз­ными физи­че­скими сер­ве­ра­ми. Про­цесс шар­динга пред­по­ла­гает раз­не­се­ния дан­ных между отдель­ными шар­дами на основе некого ключа шар­динга. Связанные одинаковым зна­че­нием ключа шар­динга сущности груп­пи­ру­ются в набор дан­ных по задан­ному клю­чу, а этот набор хра­нится в пре­де­лах одного физи­че­ского шар­да. Это суще­ственно облег­чает обра­ботку дан­ных.

Три основных составляющих шардинга:

- выбор функции шардинга,
- то, где находятся ваши данные (как вы их находите),
- то, как вы перераспределяете ваши данные.

Напри­мер, в систе­мах типа соци­аль­ных сетей клю­чом для шар­динга может быть ID поль­зо­ва­те­ля, таким обра­зом все дан­ные поль­зо­ва­теля будут хра­ниться и обра­ба­ты­ваться на одном сер­ве­ре, а не соби­раться по частям с несколь­ких. Это пример, так называемого горизонтального шардинга.

Также существует вертикальный шардниг, суть которого заключается в обычном разнесении различных таблиц по разным серверам, без партиционирования таблиц.

### Консистентный хэш

Способ создания распределенных хеш-таблиц, при котором вывод из строя одного или более серверов-хранилищ неприводит к необходимости полного переразмещения всех хранимых ключей изначений. Консистентное хеширование позволяет перераспределять только те ключи,которые использовались удаленным сервером или которые будут использоватьсяновым сервером. Таким образом, в случае выхода из строя одного узла, ключи не простопереходят на следующую ноду, а равномерно распределяются по несколькимследующим нодам.

Мы представляем, что весь диапазон нашей хэш-функции отображается не на прямую от 0 до 232 (~ 4 млрд.), а на кольцо. Т.е. у нас 4 миллиарда находится примерно там же, где 0, мы, как бы, завязываем нашу прямую.

Если мы просто используем хэш-функцию, мы должны при добавлении новых узлов все это перехэшировать. Получается так, что у нас используется остаток от деления на количество узлов.

А здесь мы не используем остаток от деления на количество узлов. Мы делаем так — мы хэш-функцию, возможно другую, применяем и к идентификатору сервера, и также располагаем сервера на этом кольце. Таким образом, получается, что каждый сервер отвечает за некий диапазон ключей после него на кольце. Соответственно, когда вы добавляете новый сервер, он забирает те диапазоны, которые находятся перед ним и после него, т.е. он частично делит диапазон. Не требуется перетасовки всего абсолютно.  

Для реализации храним хеши серверов в виде какого-либо дерева, например Red-Black. Операция поиска сервера по ключу будет занимать `O(log n)`.

http://dyagilev.org/blog/2012/03/23/consistent-hashing/

## Инвалидация кеша при репликации

https://emacsway.github.io/ru/cache-dependencies/

## Теорема CAP

Теорема САР (известная также как теорема Брюера) — эвристическое утверждение о том, что в любой реализации распределённых вычислений возможно обеспечить не более двух из трёх следующих свойств:

- **Сonsistency**(согласованность) — во всех вычислительных узлах в один момент времени данные не противоречат друг другу;
- **Availability**(доступность)— любой запрос к распределённой системе завершается корректным откликом, однако без гарантии, что ответы всех узлов системы совпадают;
- **Partition Tolerance (Устойчивость к разделению системы).** Потеря сообщений между компонентами системы (возможно даже потеря всех сообщений) не влияет на работоспособность системы. Здесь очень важный момент состоит в том, что если какие-то компоненты выходят из строя, то это тоже подпадает под этот случай, так как можно считать, что данные компоненты просто теряют связь со всей остальной системой.

В теореме говорится, что если вы строите распределенную систему, то можете удовлетворить только два из вышеупомянутых свойства, т.е. обязательно надо пожертвовать одним из свойств. Главный вопрос, который вызывает недопонимание — это что значит пожертвовать Partition Tolerance.

Не требовать partition tolerance для распределённой системы означает работу в сети, гарантирующей никогда не терять (или даже задерживать) сообщения, и чьи узлы гарантированно никогда не падают. Вы и я не работаем с такими системами, потому что их не существует.

Другими словами, требование partition tolerance для распределённой системы — это не выбор инженера, а просто практическая данность, и выбирать остаётся только между consistency и availability. Даже если инженер упрямо решит построить систему с расчётом на 100% надёжность сети, то при следующем сбое она либо потеряет данные и станет неконсистентной, или не сможет вернуть ответ на запрос, и будет недоступной.

Еще другими словами: Если у вас есть место куда вы пишите и читаете, то в случае сетевого сбоя, вам нужно выбрать либо вы продолжаете читать и писать(availibility) либо вам нужны консистентные данные.

Таким образом, так как нельзя пожертвовать Partition Tolerance, для себя я формулирую CAP теорему следующим образом. При построении распределенной системы, которая могла бы пережить отказ некоторых из ее компонент необходимо пожертвовать либо доступностью (avalability), либо согласованностью (consistency).

http://softwaremaniacs.org/blog/2012/01/16/partition-tolerance/