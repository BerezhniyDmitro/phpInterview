# Распределенные БД

## Репликация и шардинг и партиционирование

### Пар­ти­ци­о­ни­ро­ва­ние (partitioning)

Пар­ти­ци­о­ни­ро­ва­ние(также секционирование) — это раз­би­е­ние таб­лиц, содер­жа­щих боль­шое коли­че­ство запи­сей, на логи­че­ские части по неким выбран­ным адми­ни­стра­то­ром кри­те­ри­ям. Пар­ти­ци­о­ни­ро­ва­ние таб­лиц делит весь объем опе­ра­ций по обра­ботке дан­ных на несколько неза­ви­си­мых и парал­лельно выпол­ня­ю­щихся пото­ков, что суще­ственно уско­ряет работу СУБД. Для пра­виль­ного кон­фи­гу­ри­ро­ва­ния пара­мет­ров пар­ти­ци­о­ни­ро­ва­ния необ­хо­ди­мо, чтобы в каж­дом потоке было при­мерно оди­на­ко­вое коли­че­ство запи­сей.

*Напри­мер, на новост­ных сай­тах имеет смысл пар­ти­ци­о­ни­ро­вать записи по дате пуб­ли­ка­ции, так как све­жие ново­сти на несколько поряд­ков более вос­тре­бо­ваны и чаще тре­бу­ется работа именно с ними, а не со всех архи­вом за годы суще­ство­ва­ния новост­ного ресурса.*

### Репли­ка­ция (replication)

Репли­ка­ция — это син­хрон­ное или асин­хрон­ное копи­ро­ва­ние дан­ных между несколь­кими сер­ве­ра­ми. Веду­щие сер­вера назы­вают масте­рами (master), а ведо­мые сер­вера — слэй­вами (slave). Мастера исполь­зу­ются для изме­не­ния дан­ных, а слэйвы — для счи­ты­ва­ния. В клас­си­че­ской схеме репли­ка­ции обычно один мастер и несколько слэй­вов, так как в боль­шей части веб-про­ек­тов опе­ра­ций чте­ния на несколько поряд­ков боль­ше, чем опе­ра­ций запи­си. Однако в более слож­ной схеме репли­ка­ции может быть и несколько масте­ров.

Напри­мер, созда­ние несколь­ких допол­ни­тель­ных slave-сер­ве­ров поз­во­ляет снять с основ­ного сер­вера нагрузку и повы­сить общую про­из­во­ди­тель­ность систе­мы, а также можно орга­ни­зо­вать слэйвы под кон­крет­ные ресур­соём­кие задачи и таким обра­зом, напри­мер, упро­стить состав­ле­ние серь­ёз­ных ана­ли­ти­че­ских отчётов — исполь­зу­е­мый для этих целей slave может быть нагру­жен на 100%, но на работу дру­гих поль­зо­ва­те­лей при­ло­же­ния это не повли­я­ет.![](../../media/replicationHl.jpg)

Существует также схема Master-Master в ней любой из серверов может использоваться как для чтения так и для записи. Но вероятные поломки делают Master-Master репликацию непривлекательной. Выход из строя одного из серверов практически всегда приводит к потере каких-то данных. Последующее восстановление также сильно затрудняется необходимостью ручного анализа данных, которые успели либо не успели скопироваться.

Следует отметить, что репликация сама по себе не очень удобный механизм масштабирования. Причиной тому — рассинхронизация данных и задержки в копировании с мастера на слейв. Зато это отличное средство для обеспечения отказоустойчивости. Вы всегда можете переключиться на слейв, если мастер ломается и наоборот. Чаще всего репликация используется совместно с шардингом именно из соображений надежности.

#### Проблемы, решаемые репликацией

- **Распространение данных**. Обычно репликация в MySQL потребляет не очень большую часть пропускной способности сети 1 , к тому же ее можно в любой момент остановить и затем возобновить. Это полезно, если хранение копии данных происходит в географически удаленном пункте, например в другом центре обработки данных. Удаленный подчиненный сервер может работать даже с непостоянным (намеренно или по другим причинам) соединением. Однако если вы хотите обеспечить минимальное отставание реплики, то следует использовать надежный канал с малым временем задержки.
- **Балансировка нагрузки**. С помощью репликации можно распределить запросы на чтение между несколькими серверами MySQL; в приложениях с интенсивным чтением эта тактика работает очень хорошо. Реализовать несложное балансирование нагрузки можно, внеся совсем немного изменений в код. Для небольших приложений достаточно просто «зашить» в программу несколько доменных имен или воспользоваться циклическим (round-robin) разрешением DNS-имен (когда с однимдоменным именем связано несколько IP-адресов). Возможны и более изощренные решения. Стандартные технологии балансирования нагрузки, в частности сетевые балансировщики, прекрасно послужат для распределения нагрузки между несколькими серверами MySQL.
- **Резервное копирование**.Репликация – это ценное подспорье для резервного копирования.Однако подчиненный сервер все же не может использоваться в качестве резервной копии и не является заменой настоящему резервному копированию.
- **Высокая доступность и аварийное переключение на резервный сервер(failover)**. Репликация позволяет исправить ситуацию, при которой сервер MySQL является единственной точкой отказа приложения. Хорошая сис тема аварийного переключения при отказе, имеющая в составе реплицированные подчиненные серверы, способна существенно сократить время простоя. 
- **Тестирование новых версий MySQL**. Очень часто на подчиненный сервер устанавливают новую версию MySQL и перед тем как ставить ее на промышленные серверы, проверяют, что все запросы работают нормально.

####  Как работает репликация

- Главный сервер записывает изменения данных в двоичный журнал.Эти записи называются событиями двоичного журнала.  Бывает покомандная и построчная, Mysql может сама выбирать какую заюзать в конкретном случае.
  - **Покомандная**(логическая) -  протоколируются все выполненные главным сервером команды изменения данных. Когда подчиненный сервер читает из своего журнала ретрансляции событие и воспроизводит его, на самом деле он отрабатывает в точности ту же команду, которая была ранее выполнена на главном сервере. В этом есть как плюсы так и минусы. Некоторые изменения, хранящиеся в формате построчной репликации, MySQL воспроизводит более эффективно, так как ему не приходится повторять запросы, выполненные на главном сервере. А ведь воспроизведение определенных запросов обходится весьма дорого.
  - **Построчная**(фиическая) - в двоичный журнал записывает фактические изменения данных Самое существенное достоинство заключается в том, что теперь MySQL может корректно реплицировать любую команду, причем в некоторых случаях это происходит гораздо более эффективно. Основной недостаток – это то, что двоичный журнал стал намного больше и из него непонятно, какие команды привели к обновлению данных, так что использовать его для аудита с помощью программы mysqlbinlog уже невозможно. Команды не включаются в журнал событий, поэтому будет сложно определить, какая команда выполнялась. Во многих случаях знать это так же важно, как и знать об изменении строк. актически процесс применения изменений при построчной репликации в значительной степени является черным ящиком — не видно, что делает сервер. Кроме того, он плохо документирован и объяснен, поэтому, когда что-то работает неправильно, устранить неполадки довольно сложно. Например, если подчиненный сервер выберет неэффективный способ поиска строк для изменения, вы этого не заметите.
- Подчиненный сервер копирует события двоичного журнала в свой журнал ретрансляции (relay log).
- Подчиненный сервер воспроизводит события из журнала ретрансляции, применяя изменения к собственным данным.

  ![](../../media/masterSlaveReplication.png)

#### Синхронная и асинхронная

![img](http://highload.guide/blog/uploads/images_replication/replication_mysql_3.jpg)

Синхронная означает, если у вас commit прошел, то база вам обещает, что не просто commit прошел, а он еще и прошел на удаленных нодах. Т.е. мы commit сделали, подключились к одной конкретной ноде, к одному конкретному серверу, эти данные в идеале, чтобы обеспечить и масштабируемость по чтениям и доступность данных, должны улететь на соседние машины, и транзакции там должны зафиксироваться. Это синхронная репликация.

Не очень интересен вариант под названием "асинхронная репликация", потому что гарантий нет, совсем никаких. Но ок, транзакция зафиксировалась локально, это тебе что-нибудь гарантирует? Правильный ответ – нет. Потому что возможно, что все остальные реплики лежат дохлые, и те данные, которые вы вкатили на master, они у вас никуда не резервируются. Если у вас master просто упадет, в смысле SQL-сервер упадет, то все хорошо условно, он эти изменения с Write-Ahead Log'a проиграет, но если у вас внезапно демоническая сила подожжет сервер, и он сгорит, прям весь, то данные, которые вы якобы зафиксировали, они на самом деле зафиксировались на одной машине.

Синхронность – хорошо с точки зрения надежности, но медленно. Асинхронность полная – зафиксировали локально транзакцию и не ждем вообще удаленные реплики – очевидно быстро (ничем же не отличается от локального commit'a), но никаких гарантий по надежности.

Возникает промежуточный вариант – полусинхронная репликация – это когда commit возвращает успех, в тот момент, когда локально транзакция уже зафиксирована, селекты к master'у уже начнут возвращать новое состояние, новый баланс, и удаленные сервера уже скачали эти данные, уже скачали эту транзакцию, но возможно еще не успели накатить, то ли накатят через 2 секунды, то ли через 2 недели – как повезет. Это самый важный момент, с которым на практике придется сталкиваться, и делать выбор, настраивать и т.д.

### Шардинг (sharding)

Шардинг — это прием, который позволяет рас­пре­де­лять дан­ные между раз­ными физи­че­скими сер­ве­ра­ми. Процесс шар­динга пред­по­ла­гает раз­не­се­ния дан­ных между отдель­ными шар­дами на основе некого ключа шардинга. Связанные одинаковым зна­че­нием ключа шар­динга сущности груп­пи­ру­ются в набор дан­ных по задан­ному клю­чу, а этот набор хра­нится в пре­де­лах одного физи­че­ского шар­да. Это суще­ственно облег­чает обра­ботку дан­ных.

Самая важная и сложная задача, возникающая при шардинге – поиск и выборка данных. Способ поиска зависит от того, как они шардированы. Наша цель состоит в том, чтобы самые важные и часто встречающиеся запросы затрагивали как можно меньше секций. В этом плане оченьважно выбрать ключ (или ключи) секционирования данных. Ключ шардирования определяет, в какую секцию попадает та или иная строка. Если известен ключ шардирования некоторого объекта, то можно ответить на два вопроса:

- Где следует сохранить данные?
- Где найти запрошенные данные?

Запросы к нескольким шарданм хуже, чем к одному, но если они затрагивают не слишком много шардов, то все еще не так плохо. Самый худший случай – когда у вас нет ни малейшего представления о том, где находятся данные, и приходится просматривать все шарды без исключения.

#### Выбор ключа шардирования

Обычно хорошим ключом секционирования является идентификатор какой-нибудь важной сущности в базе данных. Такие сущности называются единицами шардирвания (unit of sharding). Например, если
информация секционируется по идентификатору пользователя или клиента, то единицей секционирования является соответственно пользователь или клиент.

Если модель данных сложна, то и секционировать ее труднее. Во многих приложениях существует более одного ключа секционирования, особенно если в данных можно выделить несколько важных «измерений». Иными словами, приложение должно иметь возможность эффективно взглянуть на информацию под разными углами зрения и получить целостное представление. Это означает, что некоторые данные придется хранить в двух разных видах.

#### Поиск данных

### Распределение данных

Существует два основных способа распределения данных по секциям: фиксированное и динамическое. Для обеих стратегий необходима функция секционирования, которая принимает на входе ключ секционирования строки и возвращает номер секции, в которой эта строка находится. Здесь слово «функция» употребляется в математическом смысле как отображение множества входных значений (области определения) во множество выходных значений (область значений), такую функцию можно реализовать разными способами, в том числе с использованием справочной таблицы в базе данных.

##### Фиксированное распределение

Для фиксированного распределения применяется функция разбиения, которая зависит только от значения ключа секционирования. В качестве примеров можно привести деление по модулю или хеш-функции.Такие функции отображают значения ключей секционирования на конечное число «ячеек» (buckets), в которых хранятся данные.
Пусть имеется всего 100 ячеек и требуется найти, в какую из них поместить пользователя 111. С применением деления по модулю ответ ясен:остаток от деления 111 на 100 равен 11, поэтому пользователь должен находиться в секции 11.

Однако у этой стратегии есть и недостатки.

- • Если секции велики и их немного, то балансировать нагрузку между ними будет сложно.
- • При фиксированном распределении вы лишены возможности решать, куда помещать конкретную запись, а это важно в приложениях, где нагрузка на единицы секционирования неравномерна. Некоторые части данных используются гораздо чаще, чем остальные,и когда такие обращения по большей части адресуются к одной
- и той же секции, стратегия фиксированного распределения не позволяет снять нагрузку, переместив часть данных в другую секцию.Эта проблема не так серьезна, если элементы данных малы, но их количество в каждой секции велико; закон больших чисел все расставит по своим местам.
- • Как правило, изменить алгоритм секционирования сложнее, потому что требуется перераспределить все существующие данные. Например, если секционирование производилось делением по модулю
- 10, то имеется 10 секций. Когда приложение вырастет, и секции станут слишком большими, возникнет желание увеличить их количество до 20. Но для этого придется перехешировать все заново, обновить очень много данных и перераспределить их по новым секциям.

##### Динамическое распределение

Альтернативой фиксированному распределению является динамическое распределение, описание которого хранится отдельно в виде отображения единицы секционирования на номер секции. Примером мо-
жет служить таблица с двумя столбцами: идентификатор пользователя и идентификатор секции:

``` sql
CREATE TABLE user_to_shard (
  user_id INT NOT NULL,
  shard_id INT NOT NULL,
  PRIMARY KEY (user_id)
);
```
Функцией разбиения служит сама таблица. Зная ключ секционирования (идентификатор пользователя), можно найти соответствующий номер секции. Если подходящей строки не существует, можно выбрать нужную секцию и добавить строку в таб лицу. Впоследствии сопоставление можно будет изменить, потому стратегия и называется динамической.

С динамическим распределением связаны определенные накладные расходы, так как требуется обращение к внешнему ресурсу, например серверу каталогов (узлу, на котором хранится отображение). Для поддержания приемлемой эффективности такая архитектура часто нуждается в дополнительных слоях программного обеспечения. Например, можно использовать распределенную сис тему кэширования, в памяти
которой хранятся данные с сервера каталогов, которые на практике изменяются довольно редко.
Основное достоинство динамического распределения – более точное управление местом хранения данных. Это упрощает равномерное разделение данных по секциям и позволяет гибко адаптироваться к не-
предвиденным изменениям.

Кроме того, динамическое распределение дает возможность выстраивать многоуровневую стратегию шардировани поверх простого отображения ключей на секции. Например, можно организовать двойное отображение, при котором каждой единице секционирования сопоставляется некоторая группа (например, группа членов клуба книголюбов), а сами группы по возможности размещаются в одной секции. Это позволяет воспользоваться преимуществами близости шардов и избегать межсекционных запросов.Динамическое распределение дает возможность создавать несбалансированные секции. Это полезно, когда не все сервера одинаково мощные или когда некоторые серверы используются еще и для других целей, например для архивирования данных. Если при этом имеется еще и возможность в любой момент перебалансировать шарды, то можно поддерживать взаимно однозначное соответствие между секциями и узлами, не растрачивая впустую емкость дисков. Некоторым нравится та простота, которая свойственна хранению в каждом узле ровно одной сек
ции (но не забывайте, что имеет смысл делать шарды относительно небольшими). Динамическое распределение позволяет организовать стратегию секционирования любой сложности, фиксированное такого богатства выбора не предоставляет.

##### Комбинирование динамического и фиксированного распределений

Можно также применять комбинацию динамического и фиксированного распределения. Это часто бывает полезно, а иногда даже необходимо. Динамическое распределение хорошо работает, когда отображение
не слишком велико. С ростом количества единиц секционирования его эффективность падает.

В качестве примера рассмотрим сис тему, в которой хранятся ссылки между сайтами. В ней необходимо хранить десятки миллиардов строк, а ключом секционирования является комбинация исходного и конечного URL. На любой из двух URL могут быть сотни миллионов ссылок, поэтому ни один URL по отдельности не является достаточно избирательным. Однако невозможно хранить все комбинации исходного и конечного URL в таб лице сопоставления, поскольку их слишком много, а каждый URL занимает много места.

Одно из возможных решений – конкатенировать URL и написать хеш-функцию, отображающую получившиеся строки на фиксированное число ячеек, которые затем можно динамически отображать на шарды. Если количество ячеек достаточно велико, скажем, миллион, то в каждый шард их можно поместить довольно много. В результате мы получаем многие преимущества динамического секционирования, не заводя гигантскую таблицу отображения.

##### Явное распределение

Третья стратегия позволяет приложению явно выбрать секцию при создании строки. Если данные уже существуют, то сделать это довольно трудно, поэтому такое решение редко применяется при переработке
имеющегося приложения с учетом секционирования. И все-таки иногда оно бывает полезно. Идея в том, чтобы закодировать номер секции в идентификаторе.

#### Перераспределине данных(решардинг)

При необходимости можно переместить данные из одной секции в другую, чтобы сбалансировать нагрузку. Например, многие читатели, намверное, слышали, как разработчики больших сайтов фотогалерей или
популярных социальных сетей рассказывали о своих инструментах перемещения пользователей в другие секции. Способность перемещать данные открывает целый ряд возможностей. Например, чтобы модернизировать оборудование, можно перенести всех пользователей из старой секции в новую, не останавливая секцию целиком и не переводя ее в режим чтения.

Однако мы стараемся по возможности избегать перебалансирования, так как это может вызвать приостановку обслуживания. Из-за перемещения данных становится сложнее добавлять в приложение новые функции, поскольку их приходится учитывать в сценариях перебалансирования. Если секции не слишком велики, то прибегать к этому, возможно, и не понадобится; часто для балансирования нагрузки достаточно перенести секцию целиком, а это гораздо проще, чем перемещать часть секции (и более эффективно, если говорить о стоимости в расчете на одну строку данных).

### Консистентный хэш

Способ создания распределенных хеш-таблиц, при котором вывод из строя одного или более серверов-хранилищ неприводит к необходимости полного переразмещения всех хранимых ключей изначений. Консистентное хеширование позволяет перераспределять только те ключи,которые использовались удаленным сервером или которые будут использоватьсяновым сервером. Таким образом, в случае выхода из строя одного узла, ключи не простопереходят на следующую ноду, а равномерно распределяются по несколькимследующим нодам.

Мы представляем, что весь диапазон нашей хэш-функции отображается не на прямую от 0 до 232 (~ 4 млрд.), а на кольцо. Т.е. у нас 4 миллиарда находится примерно там же, где 0, мы, как бы, завязываем нашу прямую.

Если мы просто используем хэш-функцию, мы должны при добавлении новых узлов все это перехэшировать. Получается так, что у нас используется остаток от деления на количество узлов.

А здесь мы не используем остаток от деления на количество узлов. Мы делаем так — мы хэш-функцию, возможно другую, применяем и к идентификатору сервера, и также располагаем сервера на этом кольце. Таким образом, получается, что каждый сервер отвечает за некий диапазон ключей после него на кольце. Соответственно, когда вы добавляете новый сервер, он забирает те диапазоны, которые находятся перед ним и после него, т.е. он частично делит диапазон. Не требуется перетасовки всего абсолютно.  

Для реализации храним хеши серверов в виде какого-либо дерева, например Red-Black. Операция поиска сервера по ключу будет занимать `O(log n)`.

http://dyagilev.org/blog/2012/03/23/consistent-hashing/

## Инвалидация кеша при репликации

https://emacsway.github.io/ru/cache-dependencies/

## Теорема CAP

Теорема САР (известная также как теорема Брюера) — эвристическое утверждение о том, что в любой реализации распределённых вычислений возможно обеспечить не более двух из трёх следующих свойств:

- **Сonsistency**(согласованность) — во всех вычислительных узлах в один момент времени данные не противоречат друг другу;
- **Availability**(доступность)— любой запрос к распределённой системе завершается корректным откликом, однако без гарантии, что ответы всех узлов системы совпадают;
- **Partition Tolerance (Устойчивость к разделению системы).** Потеря сообщений между компонентами системы (возможно даже потеря всех сообщений) не влияет на работоспособность системы. Здесь очень важный момент состоит в том, что если какие-то компоненты выходят из строя, то это тоже подпадает под этот случай, так как можно считать, что данные компоненты просто теряют связь со всей остальной системой.

В теореме говорится, что если вы строите распределенную систему, то можете удовлетворить только два из вышеупомянутых свойства, т.е. обязательно надо пожертвовать одним из свойств. Главный вопрос, который вызывает недопонимание — это что значит пожертвовать Partition Tolerance.

Не требовать partition tolerance для распределённой системы означает работу в сети, гарантирующей никогда не терять (или даже задерживать) сообщения, и чьи узлы гарантированно никогда не падают. Вы и я не работаем с такими системами, потому что их не существует.

Другими словами, требование partition tolerance для распределённой системы — это не выбор инженера, а просто практическая данность, и выбирать остаётся только между consistency и availability. Даже если инженер упрямо решит построить систему с расчётом на 100% надёжность сети, то при следующем сбое она либо потеряет данные и станет неконсистентной, или не сможет вернуть ответ на запрос, и будет недоступной.

Еще другими словами: Если у вас есть место куда вы пишите и читаете, то в случае сетевого сбоя, вам нужно выбрать либо вы продолжаете читать и писать(availibility) либо вам нужны консистентные данные.

Таким образом, так как нельзя пожертвовать Partition Tolerance, для себя я формулирую CAP теорему следующим образом. При построении распределенной системы, которая могла бы пережить отказ некоторых из ее компонент необходимо пожертвовать либо доступностью (avalability), либо согласованностью (consistency).

http://softwaremaniacs.org/blog/2012/01/16/partition-tolerance/

## MapReduce

MapReduce — модель распределённых вычислений, представленная компанией Google, используемая для параллельных вычислений над очень большими, вплоть до нескольких петабайт,[1] наборами данных в компьютерных кластерах.

Работа MapReduce состоит из двух шагов: Map и Reduce, названных так по аналогии с одноименными функциями высшего порядка, map и reduce.На Map-шаге происходит предварительная обработка входных данных. Для этого один из компьютеров (называемый главным узлом — master node) получает входные данные задачи, разделяет их на части и передает другим компьютерам (рабочим узлам — worker node) для предварительной обработки.На Reduce-шаге происходит свёртка предварительно обработанных данных. Главный узел получает ответы от рабочих узлов и на их основе формирует результат — решение задачи, которая изначально формулировалась.

Преимущество MapReduce заключается в том, что он позволяет распределенно производить операции предварительной обработки и свертки. Операции предварительной обработки работают независимо друг от друга и могут производиться параллельно (хотя на практике это ограничено источником входных данных и/или количеством используемых процессоров). Аналогично, множество рабочих узлов может осуществлять свертку — для этого необходимо только чтобы все результаты предварительной обработки с одним конкретным значением ключа обрабатывались одним рабочим узлом в один момент времени. Хотя этот процесс может быть менее эффективным по сравнению с более последовательными алгоритмами, MapReduce может быть применен к большим объёмам данных, которые могут обрабатываться большим количеством серверов. Так, MapReduce может быть использован для сортировки петабайта данных, что займет всего лишь несколько часов. Параллелизм также дает некоторые возможности восстановления после частичных сбоев серверов: если в рабочем узле, производящем операцию предварительной обработки или свертки, возникает сбой, то его работа может быть передана другому рабочему узлу (при условии, что входные данные для проводимой операции доступны).