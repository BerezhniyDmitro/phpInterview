# Распределенные БД

## Репликация и шардинг и партиционирование

### Пар­ти­ци­о­ни­ро­ва­ние (partitioning)

Пар­ти­ци­о­ни­ро­ва­ние — это раз­би­е­ние таб­лиц, содер­жа­щих боль­шое коли­че­ство запи­сей, на логи­че­ские части по неким выбран­ным адми­ни­стра­то­ром кри­те­ри­ям. Пар­ти­ци­о­ни­ро­ва­ние таб­лиц делит весь объем опе­ра­ций по обра­ботке дан­ных на несколько неза­ви­си­мых и парал­лельно выпол­ня­ю­щихся пото­ков, что суще­ственно уско­ряет работу СУБД. Для пра­виль­ного кон­фи­гу­ри­ро­ва­ния пара­мет­ров пар­ти­ци­о­ни­ро­ва­ния необ­хо­ди­мо, чтобы в каж­дом потоке было при­мерно оди­на­ко­вое коли­че­ство запи­сей.

*Напри­мер, на новост­ных сай­тах имеет смысл пар­ти­ци­о­ни­ро­вать записи по дате пуб­ли­ка­ции, так как све­жие ново­сти на несколько поряд­ков более вос­тре­бо­ваны и чаще тре­бу­ется работа именно с ними, а не со всех архи­вом за годы суще­ство­ва­ния новост­ного ресурса.*

### Репли­ка­ция (replication)

Репли­ка­ция — это син­хрон­ное или асин­хрон­ное копи­ро­ва­ние дан­ных между несколь­кими сер­ве­ра­ми. Веду­щие сер­вера назы­вают масте­рами (master), а ведо­мые сер­вера — слэй­вами (slave). Мастера исполь­зу­ются для изме­не­ния дан­ных, а слэйвы — для счи­ты­ва­ния. В клас­си­че­ской схеме репли­ка­ции обычно один мастер и несколько слэй­вов, так как в боль­шей части веб-про­ек­тов опе­ра­ций чте­ния на несколько поряд­ков боль­ше, чем опе­ра­ций запи­си. Однако в более слож­ной схеме репли­ка­ции может быть и несколько масте­ров.

Напри­мер, созда­ние несколь­ких допол­ни­тель­ных slave-сер­ве­ров поз­во­ляет снять с основ­ного сер­вера нагрузку и повы­сить общую про­из­во­ди­тель­ность систе­мы, а также можно орга­ни­зо­вать слэйвы под кон­крет­ные ресур­соём­кие задачи и таким обра­зом, напри­мер, упро­стить состав­ле­ние серь­ёз­ных ана­ли­ти­че­ских отчётов — исполь­зу­е­мый для этих целей slave может быть нагру­жен на 100%, но на работу дру­гих поль­зо­ва­те­лей при­ло­же­ния это не повли­я­ет.

Существует также схема Master-Master в ней любой из серверов может использоваться как для чтения так и для записи. Но вероятные поломки делают Master-Master репликацию непривлекательной. Выход из строя одного из серверов практически всегда приводит к потере каких-то данных. Последующее восстановление также сильно затрудняется необходимостью ручного анализа данных, которые успели либо не успели скопироваться.

Следует отметить, что репликация сама по себе не очень удобный механизм масштабирования. Причиной тому — рассинхронизация данных и задержки в копировании с мастера на слейв. Зато это отличное средство для обеспечения отказоустойчивости. Вы всегда можете переключиться на слейв, если мастер ломается и наоборот. Чаще всего репликация используется совместно с шардингом именно из соображений надежности.

### Шар­динг (sharding)

Шар­динг — это при­ем, кото­рый поз­во­ляет рас­пре­де­лять дан­ные между раз­ными физи­че­скими сер­ве­ра­ми. Про­цесс шар­динга пред­по­ла­гает раз­не­се­ния дан­ных между отдель­ными шар­дами на основе некого ключа шар­динга. Связанные одинаковым зна­че­нием ключа шар­динга сущности груп­пи­ру­ются в набор дан­ных по задан­ному клю­чу, а этот набор хра­нится в пре­де­лах одного физи­че­ского шар­да. Это суще­ственно облег­чает обра­ботку дан­ных.

Напри­мер, в систе­мах типа соци­аль­ных сетей клю­чом для шар­динга может быть ID поль­зо­ва­те­ля, таким обра­зом все дан­ные поль­зо­ва­теля будут хра­ниться и обра­ба­ты­ваться на одном сер­ве­ре, а не соби­раться по частям с несколь­ких. Это пример, так называемого горизонтального шардинга.

Также существует вертикальный шардниг, суть которого заключается в обычном разнесении различных таблиц по разным серверам, без партиционирования таблиц.



## Инвалидация кеша при репликации

https://emacsway.github.io/ru/cache-dependencies/

## Консистентный хэш

Способ создания распределенных хеш-таблиц, при котором вывод из строя одного или более серверов-хранилищ неприводит к необходимости полного переразмещения всех хранимых ключей изначений. Консистентное хеширование позволяет перераспределять только те ключи,которые использовались удаленным сервером или которые будут использоватьсяновым сервером. Таким образом, в случае выхода из строя одного узла, ключи не простопереходят на следующую ноду, а равномерно распределяются по несколькимследующим нодам.

http://dyagilev.org/blog/2012/03/23/consistent-hashing/

## Теорема CAP

Теорема САР (известная также как теорема Брюера) — эвристическое утверждение о том, что в любой реализации распределённых вычислений возможно обеспечить не более двух из трёх следующих свойств:

- согласованность данных (англ. consistency) — во всех вычислительных узлах в один момент времени данные не противоречат друг другу;
- доступность (англ. availability) — любой запрос к распределённой системе завершается корректным откликом, однако без гарантии, что ответы всех узлов системы совпадают;
- устойчивость к разделению (англ. partition tolerance) — расщепление распределённой системы на несколько изолированных секций не приводит к некорректности отклика от каждой из секций.